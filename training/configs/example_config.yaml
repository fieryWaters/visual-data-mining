# Example Training Configuration

# Dataset configuration
dataset:
  name: "custom_dataset"  # Required by finetuning.py
  file: "synthetic_dataset.py"  # Which implementation file to use
  test_split: "test"  # Split to use for validation
  split_ratio: 0.8  # Train/test split ratio
  subset_size: 100  # Use this many samples (for test datasets)
  use_subset: false  # Whether to use subset (true for testing)

# Model configuration
model:
  name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  type: "vision"  # Model type (vision or text)
  peft: true  # Whether to use PEFT
  peft_method: "lora"  # PEFT method (lora, prefix, etc.)
  use_fast_kernels: true  # Whether to use fast CUDA kernels

# Checkpoint configuration
checkpoint:
  root_dir: "finetuned_model"
  dir_name: "fine-tuned"
  create_backups: true  # Whether to create backups between jobs
  backup_format: "fine-tuned_backup_job_{job_num}"  # Backup naming pattern

# Training parameters
training:
  batch_size: 8
  lr: 1e-5
  weight_decay: 0.01
  gamma: 0.85  # LR scheduler gamma
  epochs_per_job: 1
  total_jobs: 10
  gradient_accumulation_steps: 1
  run_validation: true
  enable_fsdp: true  # Whether to use FSDP
  batching_strategy: "padding"  # Strategy for batching samples
  use_wandb: true  # Whether to use Weights & Biases

# SLURM configuration
slurm:
  partition: "gpucluster"
  time: "04:00:00"
  nodes: 1
  job_name: "llama_training"
  output_path: "logs/pipeline_refactor_%j.log"
  gpu_id: 3  # CUDA_VISIBLE_DEVICES value

# Wandb configuration
wandb:
  project: "llama_recipes"  
  group_format: "training_run_{date}"  # Format for group name

# Evaluation configuration
evaluation:
  batch_size: 8
  visualize: false  # Whether to show visualizations during evaluation
  metrics: ["distance"]  # Metrics to calculate
  peft_model_path: "finetuned_model/fine-tuned/peft_weights/"  # Path to model weights
