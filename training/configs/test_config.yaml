# Test Configuration (Small Dataset)

# Dataset configuration
dataset:
  name: "custom_dataset"
  file: "test_dataset.py"  # Use test dataset implementation
  test_split: "test"
  split_ratio: 0.8
  subset_size: 100
  use_subset: true  # Use subset of data for testing

# Model configuration
model:
  name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  type: "vision"
  peft: true
  peft_method: "lora"
  use_fast_kernels: true

# Checkpoint configuration
checkpoint:
  root_dir: "finetuned_model_test"  # Separate directory for test runs
  dir_name: "fine-tuned"
  create_backups: false  # No need for backups in test mode
  backup_format: "fine-tuned_backup_job_{job_num}"

# Training parameters
training:
  batch_size: 8
  lr: 1e-5
  weight_decay: 0.01
  gamma: 0.85
  epochs_per_job: 1
  total_jobs: 1  # Only one job for testing
  gradient_accumulation_steps: 1
  run_validation: true
  enable_fsdp: true
  batching_strategy: "padding"
  use_wandb: false  # No wandb in test mode

# SLURM configuration
slurm:
  partition: "gpucluster"
  time: "01:00:00"  # Shorter time for test runs
  nodes: 1
  job_name: "llama_test"
  output_path: "logs/test_job_%j.log"
  gpu_id: 3

# Evaluation configuration
evaluation:
  batch_size: 8
  visualize: true  # Visualize results for test runs
  metrics: ["distance"]
  peft_model_path: "finetuned_model_test/fine-tuned/peft_weights/"