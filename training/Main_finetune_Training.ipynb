{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd392e-c46f-4b69-b399-937ee01b0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_model_and_processor():\n",
    "    print(\"Loading model and processor...\")\n",
    "    model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    \n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    print(\"Loading model with distributed configuration...\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        use_safetensors=True,\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Tying model weights...\")\n",
    "    if hasattr(model, 'tie_weights'):\n",
    "        model.tie_weights()\n",
    "    \n",
    "    print(\"Model and processor loaded successfully\")\n",
    "    return model, processor\n",
    "\n",
    "def format_example(example, tokenizer):\n",
    "    try:\n",
    "        bbox = example.get('bbox', [0, 0, 0, 0])\n",
    "        bbox_str = f\"x1={bbox[0]}, y1={bbox[1]}, x2={bbox[2]}, y2={bbox[3]}\"\n",
    "        \n",
    "        instruction = (\n",
    "            f\"Analyze this UI image and locate the button with text '{example.get('OCR', '')}'. \"\n",
    "            f\"The button type is {example.get('type', 'unknown')}.\"\n",
    "        )\n",
    "        \n",
    "        response = (\n",
    "            f\"The button is located at coordinates: {bbox_str}. \"\n",
    "            f\"Description: {example.get('description', 'Not provided')}. \"\n",
    "            f\"Purpose: {example.get('purpose', 'Not specified')}.\"\n",
    "        )\n",
    "        \n",
    "        text = f\"User: {instruction}\\nAssistant: {response}\"\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'][0],\n",
    "            'attention_mask': tokenized['attention_mask'][0],\n",
    "            'labels': tokenized['input_ids'][0].clone()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting example: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_dataset(tokenizer):\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"miketes/Web-filtered-english-wave-ui-25k\")\n",
    "    total_examples = len(dataset['train'])\n",
    "    \n",
    "    print(f\"Processing all {total_examples} examples...\")\n",
    "    formatted_data = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    for idx, example in enumerate(dataset['train']):\n",
    "        formatted = format_example(example, tokenizer)\n",
    "        if formatted is not None:\n",
    "            formatted_data.append(formatted)\n",
    "            processed_count += 1\n",
    "            \n",
    "            if processed_count % 1000 == 0:\n",
    "                print(f\"Successfully processed {processed_count}/{total_examples} examples\")\n",
    "    \n",
    "    print(f\"\\nTotal examples processed: {processed_count}\")\n",
    "    formatted_dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    # Create train (80%), validation (10%), and test (10%) splits\n",
    "    first_split = formatted_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = first_split['train']\n",
    "    second_split = first_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'validation': second_split['train'],\n",
    "        'test': second_split['test']\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--checkpoint_dir', type=str, required=True)\n",
    "    parser.add_argument('--start_epoch', type=int, required=True)\n",
    "    parser.add_argument('--epochs_per_job', type=int, required=True)\n",
    "    parser.add_argument('--wandb_run_id', type=str, required=True)\n",
    "    parser.add_argument('--resume_from_checkpoint', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize model and processor\n",
    "    model, processor = load_model_and_processor()\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset_splits = prepare_dataset(processor.tokenizer)\n",
    "    \n",
    "    # Configure training arguments\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=args.checkpoint_dir,\n",
    "        num_train_epochs=args.epochs_per_job,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        optim=\"adamw_torch\",\n",
    "        bf16=False,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "        max_seq_length=512\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_splits[\"train\"],\n",
    "        eval_dataset=dataset_splits[\"test\"],\n",
    "        tokenizer=processor.tokenizer,\n",
    "        dataset_text_field=\"input_ids\"\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"\\nTraining Configuration:\")\n",
    "    print(f\"Number of training examples: {len(trainer.train_dataset)}\")\n",
    "    print(f\"Number of validation examples: {len(trainer.eval_dataset)}\")\n",
    "    print(f\"Number of epochs: {args.epochs_per_job}\")\n",
    "    print(f\"Starting from epoch: {args.start_epoch}\")\n",
    "    print(f\"Checkpoint directory: {args.checkpoint_dir}\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n",
    "    \n",
    "    # Save the model after training\n",
    "    trainer.save_model(args.checkpoint_dir)\n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
