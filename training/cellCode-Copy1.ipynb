{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd2c19-058a-44b9-b0be-ba70ae9754bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached Hugging Face token\n",
      "Starting training setup...\n",
      "Loading model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 21:36:17,549 - WARNING - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d0e8806bcc498c97287f10da526de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from miketes/Web-filtered-english-wave-ui-25k\n",
      "Dataset size: 10 samples\n",
      "Preparing dataset with 10 examples...\n",
      "[79.92, 7.64, 85.31, 10.42]\n",
      "[20.47, 93.87, 40.0, 96.51]\n",
      "[85.35, 0.0, 99.02, 6.74]\n",
      "[46.25, 80.0, 60.62, 86.67]\n",
      "[5.0, 89.56, 35.0, 93.56]\n",
      "[66.21, 12.99, 75.39, 18.07]\n",
      "[23.56, 2.78, 29.98, 8.64]\n",
      "[44.53, 5.47, 49.51, 7.91]\n",
      "[48.75, 49.89, 52.03, 52.67]\n",
      "[27.64, 52.15, 40.33, 58.3]\n",
      "Dataset preparation complete. Formatted 10 examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_kwargs. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/trl/trainer/sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "2024-11-21 21:36:26,429 - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n",
      "\n",
      "Training parameters:\n",
      "Number of training examples: 10\n",
      "Batch size: 2\n",
      "Gradient accumulation steps: 8\n",
      "Effective batch size: 16\n",
      "Number of epochs: 3\n",
      "Learning rate: 1e-05\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error occurred during training: Caught KeyError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in collate_fn\n",
      "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in <listcomp>\n",
      "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n",
      "                                          ~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'messages'\n",
      "\n",
      "\n",
      "Cleaning up...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in collate_fn\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in <listcomp>\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n                                          ~~~~~~~^^^^^^^^^^^^\nKeyError: 'messages'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 224\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining interrupted by user. Saving checkpoint...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 187\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args\u001b[38;5;241m.\u001b[39mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaving final model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(OUTPUT_DIR)\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/transformers/trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[1;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/transformers/trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/accelerate/data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/923676946/git-repos/Visual-Data-Mining-AI-Model/venv_visual_data_mining/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in collate_fn\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_104381/4041941404.py\", line 96, in <listcomp>\n    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False)\n                                          ~~~~~~~^^^^^^^^^^^^\nKeyError: 'messages'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"ssh username@hpc_cluster (whatever it is for you)\n",
    "cd Visual-Data-Mining-AI-Model (after cloning and change the branch)\n",
    "create and activate the venv\n",
    "run activate_gpu.sh\n",
    "pip install -r requirements.txt\n",
    "cd training\n",
    "python3 llama_3.2_lora.py\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Constants\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "OUTPUT_DIR = \"llama-vision-ui-checkpoint-full\"\n",
    "DATASET_NAME = \"miketes/Web-filtered-english-wave-ui-25k\"\n",
    "\n",
    "def setup_model_and_processor(checkpoint_dir=None):\n",
    "    \"\"\"Setup model optimized for A100 GPU\"\"\"\n",
    "    print(\"Loading model and processor...\")\n",
    "    \n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        MODEL_ID if checkpoint_dir is None else checkpoint_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def prepare_dataset(dataset, processor):\n",
    "    \"\"\"Convert dataset to format expected by model and tokenize\"\"\"\n",
    "    print(f\"Preparing dataset with {len(dataset)} examples...\")\n",
    "    \n",
    "    def format_instruction(example):\n",
    "        instruction = f\"\"\"Return the bounding box of the {example['description']}. It's used to {example['purpose']} and if we click it {example['expectation']}.\"\"\"\n",
    "        bbox = example['bbox']\n",
    "        x_res, y_res = example['resolution']\n",
    "        #bounding boxes are in percentage of the screen\n",
    "        bbox[0] = bbox[0] / x_res * 100\n",
    "        bbox[1] = bbox[1] / y_res * 100\n",
    "        bbox[2] = bbox[2] / x_res * 100\n",
    "        bbox[3] = bbox[3] / y_res * 100\n",
    "        bbox = [round(x, 2) for x in bbox]  # Limit to two decimal places\n",
    "        print(bbox)\n",
    "        reply = f\"\"\"{str(bbox)}\"\"\"\n",
    "        \n",
    "        # Create messages format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": example['image']},\n",
    "                    {\"type\": \"text\", \"text\": instruction}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": reply}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Tokenize the messages\n",
    "        texts = processor.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = processor(\n",
    "            text=texts,\n",
    "            images=example['image'],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension since we're processing one example at a time\n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    prepared_data = []\n",
    "    for example in dataset:\n",
    "        try:\n",
    "            formatted = format_instruction(example)\n",
    "            prepared_data.append(formatted)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping example due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Dataset preparation complete. Formatted {len(prepared_data)} examples.\")\n",
    "    return prepared_data\n",
    "\n",
    "def create_data_collator(processor):\n",
    "    \"\"\"Create a data collator that handles images and text\"\"\"\n",
    "    def collate_fn(examples):\n",
    "        texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) \n",
    "                for example in examples]\n",
    "        images = [example[\"messages\"][0][\"content\"][0][\"image\"] \n",
    "                 for example in examples]\n",
    "        \n",
    "        batch = processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n",
    "        labels[labels == image_token_id] = -100\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "    return collate_fn\n",
    "\n",
    "def train(resume_from_checkpoint=None):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"Starting training setup...\")\n",
    "    \n",
    "    model, processor = setup_model_and_processor(resume_from_checkpoint)\n",
    "    \n",
    "    print(f\"Loading dataset from {DATASET_NAME}\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    \n",
    "    MAX_SAMPLES = 10  # testing\n",
    "    dataset = dataset.select(range(min(len(dataset), MAX_SAMPLES)))\n",
    "    print(f\"Dataset size: {len(dataset)} samples\")\n",
    "    \n",
    "    # Pass processor to prepare_dataset\n",
    "    prepared_dataset = prepare_dataset(dataset, processor)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=1e-5,\n",
    "        bf16=True,\n",
    "        tf32=False,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",  # Changed from \"tensorboard\" to \"none\"\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        max_steps=5000,\n",
    "        dataloader_num_workers=4,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "        weight_decay=0.01,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        bf16_full_eval=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        group_by_length=True,\n",
    "    )\n",
    "\n",
    "    print(\"Creating trainer...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=prepared_dataset,\n",
    "        data_collator=create_data_collator(processor),\n",
    "        tokenizer=processor.tokenizer,\n",
    "        dataset_kwargs={'skip_prepare_dataset': True}  \n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining parameters:\")\n",
    "    print(f\"Number of training examples: {len(prepared_dataset)}\")\n",
    "    print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    \n",
    "    print(\"\\nSaving final model...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler(os.path.join(OUTPUT_DIR, 'training.log'))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    token = os.getenv('HF_TOKEN')\n",
    "    if token:\n",
    "        from huggingface_hub import login\n",
    "        login(token=token)\n",
    "    else:\n",
    "        if HfFolder.get_token() is not None:\n",
    "            print(\"Using cached Hugging Face token\")\n",
    "        else:\n",
    "            print(\"No token found. Please set HF_TOKEN environment variable or login manually first\")\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_path = os.path.join(OUTPUT_DIR, latest_checkpoint)\n",
    "            print(f\"Found checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        train(resume_from_checkpoint=checkpoint_path)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user. Saving checkpoint...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred during training: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        print(\"\\nCleaning up...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e9656-ac00-4580-9828-6aa813728147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
