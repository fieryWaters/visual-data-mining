{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad15336-369d-4577-9f53-1779c97163a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, Dataset\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def setup_distributed():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "\n",
    "def load_model_and_processor():\n",
    "    print(\"Loading model and processor...\")\n",
    "    model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    \n",
    "    print(\"Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=True,\n",
    "        offload_folder=\"offload\",\n",
    "        offload_state_dict=True,\n",
    "    )\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def format_example(example, tokenizer):\n",
    "    try:\n",
    "        bbox = example.get('bbox', [0, 0, 0, 0])\n",
    "        bbox_str = f\"x1={bbox[0]}, y1={bbox[1]}, x2={bbox[2]}, y2={bbox[3]}\"\n",
    "        \n",
    "        instruction = (\n",
    "            f\"Analyze this UI image and locate the button with text '{example.get('OCR', '')}'. \"\n",
    "            f\"The button type is {example.get('type', 'unknown')}.\"\n",
    "        )\n",
    "        \n",
    "        response = (\n",
    "            f\"The button is located at coordinates: {bbox_str}. \"\n",
    "            f\"Description: {example.get('description', 'Not provided')}. \"\n",
    "            f\"Purpose: {example.get('purpose', 'Not specified')}.\"\n",
    "        )\n",
    "        \n",
    "        text = f\"User: {instruction}\\nAssistant: {response}\"\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'][0],\n",
    "            'attention_mask': tokenized['attention_mask'][0],\n",
    "            'labels': tokenized['input_ids'][0].clone()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting example: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_dataset(tokenizer):\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"miketes/Web-filtered-english-wave-ui-25k\")\n",
    "    total_examples = len(dataset['train'])\n",
    "    \n",
    "    print(f\"Processing all {total_examples} examples...\")\n",
    "    formatted_data = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    for idx, example in enumerate(dataset['train']):\n",
    "        formatted = format_example(example, tokenizer)\n",
    "        if formatted is not None:\n",
    "            formatted_data.append(formatted)\n",
    "            processed_count += 1\n",
    "            \n",
    "            if processed_count % 1000 == 0:\n",
    "                print(f\"Successfully processed {processed_count}/{total_examples} examples\")\n",
    "    \n",
    "    print(f\"\\nTotal examples processed: {processed_count}\")\n",
    "    formatted_dataset = Dataset.from_list(formatted_data)\n",
    "    \n",
    "    # Create train (80%), validation (10%), and test (10%) splits\n",
    "    first_split = formatted_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = first_split['train']\n",
    "    second_split = first_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    return {\n",
    "        'train': train_dataset,\n",
    "        'validation': second_split['train'],\n",
    "        'test': second_split['test']\n",
    "    }\n",
    "\n",
    "class LlamaTrainer:\n",
    "    def __init__(self, model, optimizer, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        self.model.train()\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        self.model.eval()\n",
    "        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "        \n",
    "        return outputs.loss.item()\n",
    "\n",
    "    def save_checkpoint(self, epoch, step, loss, checkpoint_dir):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'step': step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        path = os.path.join(checkpoint_dir, f'checkpoint-{epoch}-{step}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        return path\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['step'], checkpoint['loss']\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--checkpoint_dir', type=str, required=True)\n",
    "    parser.add_argument('--start_epoch', type=int, required=True)\n",
    "    parser.add_argument('--epochs_per_job', type=int, required=True)\n",
    "    parser.add_argument('--wandb_run_id', type=str, required=True)\n",
    "    parser.add_argument('--resume_from_checkpoint', type=str, default=None)\n",
    "    parser.add_argument('--local_rank', type=int, default=-1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize distributed training\n",
    "    setup_distributed()\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if dist.get_rank() == 0:\n",
    "        wandb.init(project=\"llama_vision_training\", id=args.wandb_run_id, resume=\"allow\")\n",
    "\n",
    "    # Initialize model and processor\n",
    "    model, processor = load_model_and_processor()\n",
    "    model = model.to(torch.cuda.current_device())\n",
    "    model = DDP(model, device_ids=[torch.cuda.current_device()])\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset_splits = prepare_dataset(processor.tokenizer)\n",
    "    \n",
    "    # Create data loaders with distributed sampling\n",
    "    train_sampler = DistributedSampler(dataset_splits['train'])\n",
    "    train_loader = DataLoader(\n",
    "        dataset_splits['train'],\n",
    "        batch_size=1,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset_splits['validation'],\n",
    "        batch_size=1,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    num_training_steps = len(train_loader) * args.epochs_per_job\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = LlamaTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=torch.cuda.current_device()\n",
    "    )\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    start_epoch = args.start_epoch\n",
    "    global_step = 0\n",
    "    if args.resume_from_checkpoint:\n",
    "        start_epoch, global_step, _ = trainer.load_checkpoint(args.resume_from_checkpoint)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, start_epoch + args.epochs_per_job):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_steps = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            loss = trainer.train_step(batch)\n",
    "            train_loss += loss\n",
    "            train_steps += 1\n",
    "            \n",
    "            if (step + 1) % 16 == 0:  # Gradient accumulation\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if dist.get_rank() == 0:\n",
    "                    wandb.log({\n",
    "                        'train_loss': train_loss / train_steps,\n",
    "                        'learning_rate': scheduler.get_last_lr()[0],\n",
    "                        'epoch': epoch,\n",
    "                        'global_step': global_step\n",
    "                    })\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Save checkpoint every 50 steps\n",
    "            if dist.get_rank() == 0 and global_step % 50 == 0:\n",
    "                trainer.save_checkpoint(epoch, global_step, loss, args.checkpoint_dir)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                loss = trainer.eval_step(batch)\n",
    "                val_loss += loss\n",
    "                val_steps += 1\n",
    "        \n",
    "        if dist.get_rank() == 0:\n",
    "            wandb.log({\n",
    "                'val_loss': val_loss / val_steps,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "            \n",
    "            # Save epoch checkpoint\n",
    "            trainer.save_checkpoint(epoch, global_step, val_loss / val_steps, args.checkpoint_dir)\n",
    "\n",
    "    if dist.get_rank() == 0:\n",
    "        wandb.finish()\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
