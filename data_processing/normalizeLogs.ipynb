{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600d5914",
   "metadata": {},
   "source": [
    "# Normalization Process Documentation\n",
    "\n",
    "This notebook normalizes the mining data collected in the `logs` directory:\n",
    "\n",
    "1. **For Screenshots**:\n",
    "   - Resizes images to max 1120x1120 pixels while preserving aspect ratio\n",
    "   - Keeps original filenames\n",
    "   - Only processes screenshots taken during user activity (within 15 seconds of events)\n",
    "   \n",
    "2. **For JSON Files**:\n",
    "   - Normalizes click coordinates to 0-1 range (if not already normalized)\n",
    "   - Normalizes by dividing x by screen width and y by screen height\n",
    "   - Preserves all other data\n",
    "   \n",
    "3. **Output Structure**:\n",
    "   - Saves to a specific folder in `../data/normalized/[hostname]_[timestamp]/`\n",
    "   - Maintains the same folder structure as the original logs directory\n",
    "   - Copies session_prompts.log if it exists\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Run all cells in order\n",
    "2. The normalized data will be saved to a new directory in `../data/normalized/`\n",
    "3. Each normalization run creates a subfolder named `[hostname]_[timestamp]`\n",
    "\n",
    "You can now use this normalized data with your analysis or training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4270f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import pyautogui\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a982a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen size: 1440x900\n",
      "Input: /Users/jacob/git-repos/visual-data-mining/mining/logs_jacob_may_6_2025\n",
      "Output: /Users/jacob/git-repos/visual-data-mining/data/normalized_my_mac_20250509_005206\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "print(f\"Screen size: {screen_width}x{screen_height}\")\n",
    "\n",
    "# Define paths\n",
    "ROOT_DIR = os.path.abspath('../')\n",
    "MINING_DIR = os.path.join(ROOT_DIR, 'mining')\n",
    "# LOGS_DIR = os.path.join(MINING_DIR, 'logs')\n",
    "LOGS_DIR = os.path.join(ROOT_DIR, 'data', 'old 5-1-2025')\n",
    "SCREENSHOTS_DIR = os.path.join(LOGS_DIR, 'screenshots')\n",
    "JSON_DIR = os.path.join(LOGS_DIR, 'sanitized_json')\n",
    "\n",
    "# Generate distinctive subfolder name with timestamp and hostname\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "hostname = socket.gethostname().replace(\".\", \"_\")\n",
    "NORMALIZED_SUBFOLDER = f\"{hostname}_{timestamp}\"\n",
    "\n",
    "# Set up output directory structure\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, 'data', 'normalized', NORMALIZED_SUBFOLDER)\n",
    "OUTPUT_SCREENSHOTS_DIR = os.path.join(OUTPUT_DIR, 'screenshots')\n",
    "OUTPUT_JSON_DIR = os.path.join(OUTPUT_DIR, 'sanitized_json')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_SCREENSHOTS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {LOGS_DIR}\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75574bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core functions for normalization and filtering\n",
    "def normalize_image_size(img, max_size=(1120, 1120)):\n",
    "    \"\"\"Resize an image to a maximum size while preserving aspect ratio.\"\"\"\n",
    "    original_width, original_height = img.size\n",
    "    resize_ratio = min(max_size[0] / original_width, max_size[1] / original_height)\n",
    "    \n",
    "    if resize_ratio < 1:\n",
    "        new_width, new_height = int(original_width * resize_ratio), int(original_height * resize_ratio)\n",
    "        return img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    return img\n",
    "\n",
    "def extract_activity_timestamps(filepath):\n",
    "    \"\"\"Extract timestamps of all user activity events from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        timestamps = []\n",
    "        for event in data.get('events', []):\n",
    "            ts = event.get('timestamp')\n",
    "            if not ts:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(ts, str):\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
    "                    timestamps.append(dt.timestamp())\n",
    "                except:\n",
    "                    pass\n",
    "            elif isinstance(ts, (int, float)):\n",
    "                timestamps.append(ts / 1000 if ts > 1e10 else ts)\n",
    "                \n",
    "        return timestamps\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting timestamps from {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "def merge_time_ranges(ranges):\n",
    "    \"\"\"Merge overlapping time ranges.\"\"\"\n",
    "    if not ranges:\n",
    "        return []\n",
    "    \n",
    "    sorted_ranges = sorted(ranges)\n",
    "    merged = [sorted_ranges[0]]\n",
    "    \n",
    "    for current in sorted_ranges[1:]:\n",
    "        last = merged[-1]\n",
    "        if current[0] <= last[1]:\n",
    "            merged[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def extract_timestamp_from_filename(filename):\n",
    "    \"\"\"Extract timestamp from screenshot filename.\"\"\"\n",
    "    match = re.search(r'screen_(\\d{8})_(\\d{6})_\\d+\\.jpg', filename)\n",
    "    if match:\n",
    "        dt = datetime.strptime(f\"{match.group(1)}_{match.group(2)}\", \"%Y%m%d_%H%M%S\")\n",
    "        return dt.timestamp()\n",
    "    return None\n",
    "\n",
    "def is_in_activity_range(timestamp, activity_ranges):\n",
    "    \"\"\"Check if a timestamp falls within any activity range.\"\"\"\n",
    "    return any(start <= timestamp <= end for start, end in activity_ranges)\n",
    "\n",
    "def process_screenshot(filepath):\n",
    "    \"\"\"Process a single screenshot file and save it to the output directory.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(filepath)\n",
    "        output_path = os.path.join(OUTPUT_SCREENSHOTS_DIR, filename)\n",
    "        \n",
    "        img = Image.open(filepath)\n",
    "        normalized_img = normalize_image_size(img)\n",
    "        normalized_img.save(output_path, quality=95)\n",
    "        \n",
    "        return {\"status\": \"success\", \"filename\": filename}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"filename\": os.path.basename(filepath), \"error\": str(e)}\n",
    "\n",
    "def process_json_file(filepath):\n",
    "    \"\"\"Process a single JSON file and save it to the output directory.\"\"\"\n",
    "    try:\n",
    "        filename = os.path.basename(filepath)\n",
    "        output_path = os.path.join(OUTPUT_JSON_DIR, filename)\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        file_needed_normalization = False\n",
    "        events_normalized = 0\n",
    "        \n",
    "        for event in data.get('events', []):\n",
    "            if event.get('event') == 'MOUSE':\n",
    "                for coord in ['x', 'y']:\n",
    "                    if coord in event:\n",
    "                        value = event[coord]\n",
    "                        if isinstance(value, int) or value > 1:\n",
    "                            file_needed_normalization = True\n",
    "                            events_normalized += 1\n",
    "                            event[coord] = value / (screen_width if coord == 'x' else screen_height)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"filename\": filename, \n",
    "            \"normalized\": file_needed_normalization,\n",
    "            \"events_normalized\": events_normalized\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"filename\": os.path.basename(filepath), \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feccf31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 cores for parallel processing\n",
      "Analyzing user activity from 2222 JSON files...\n",
      "Extracting activity timestamps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 2222/2222 [00:00<00:00, 2614.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196451 activity events\n",
      "Created 371 activity ranges after merging\n",
      "Total activity time: 15.29 hours\n",
      "Pre-filtering screenshots based on activity...\n",
      "Found 378469 total screenshots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering screenshots: 100%|██████████| 378469/378469 [00:10<00:00, 36050.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 131385 screenshots (keeping 34.71%)\n",
      "Processing 131385 filtered screenshots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing screenshots (num_proc=4):   5%|▌         | 6916/131385 [03:51<1:09:23, 29.90 examples/s]\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:714\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/multiprocess/managers.py:821\u001b[39m, in \u001b[36mBaseProxy._callmethod\u001b[39m\u001b[34m(self, methodname, args, kwds)\u001b[39m\n\u001b[32m    820\u001b[39m conn.send((\u001b[38;5;28mself\u001b[39m._id, methodname, args, kwds))\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m kind, result = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kind == \u001b[33m'\u001b[39m\u001b[33m#RETURN\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/multiprocess/connection.py:253\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/multiprocess/connection.py:433\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/multiprocess/connection.py:398\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m json_dataset = Dataset.from_dict({\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m: json_files})\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_screenshot_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filtered screenshots...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m screenshot_results = \u001b[43mscreenshots_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_screenshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfilepath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_cores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing screenshots\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     46\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(json_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m JSON files...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m json_results = json_dataset.map(\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m example: process_json_file(example[\u001b[33m\"\u001b[39m\u001b[33mfilepath\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     51\u001b[39m     num_proc=num_cores, batched=\u001b[38;5;28;01mFalse\u001b[39;00m, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing JSON files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3171\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3165\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3167\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3168\u001b[39m     total=pbar_total,\n\u001b[32m   3169\u001b[39m     desc=(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3170\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3171\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[32m   3173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3174\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3175\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:728\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    727\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-repos/visual-data-mining/mining/.venv/lib/python3.12/site-packages/multiprocess/pool.py:770\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28mself\u001b[39m.wait(timeout)\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ready():\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._success:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mTimeoutError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Execute the optimized normalization process\n",
    "num_cores = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {num_cores} cores for parallel processing\")\n",
    "\n",
    "# First, extract activity timestamps and build activity ranges\n",
    "print(f\"Analyzing user activity from {len(json_files := glob.glob(os.path.join(JSON_DIR, '*.json')))} JSON files...\")\n",
    "\n",
    "# Extract activity timestamps and create ranges\n",
    "print(\"Extracting activity timestamps...\")\n",
    "all_timestamps = []\n",
    "for filepath in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "    all_timestamps.extend(extract_activity_timestamps(filepath))\n",
    "\n",
    "print(f\"Found {len(all_timestamps)} activity events\")\n",
    "time_ranges = [(t - 15, t + 15) for t in all_timestamps]  # 15-second buffer\n",
    "merged_ranges = merge_time_ranges(time_ranges)\n",
    "print(f\"Created {len(merged_ranges)} activity ranges after merging\")\n",
    "\n",
    "# Calculate total activity time\n",
    "total_activity_seconds = sum(end - start for start, end in merged_ranges)/3600\n",
    "print(f\"Total activity time: {total_activity_seconds:.2f} hours\")\n",
    "\n",
    "# Pre-filter screenshot files\n",
    "print(\"Pre-filtering screenshots based on activity...\")\n",
    "screenshot_files = glob.glob(os.path.join(SCREENSHOTS_DIR, \"*.jpg\"))\n",
    "print(f\"Found {len(screenshot_files)} total screenshots\")\n",
    "\n",
    "filtered_screenshot_files = []\n",
    "for filepath in tqdm(screenshot_files, desc=\"Filtering screenshots\"):\n",
    "    filename = os.path.basename(filepath)\n",
    "    timestamp = extract_timestamp_from_filename(filename)\n",
    "    if timestamp is not None and is_in_activity_range(timestamp, merged_ranges):\n",
    "        filtered_screenshot_files.append(filepath)\n",
    "\n",
    "filtering_ratio = len(filtered_screenshot_files)/len(screenshot_files)*100\n",
    "print(f\"Filtered to {len(filtered_screenshot_files)} screenshots (keeping {filtering_ratio:.2f}%)\")\n",
    "\n",
    "# Process filtered screenshots and JSON files in parallel\n",
    "screenshots_dataset = Dataset.from_dict({\"filepath\": filtered_screenshot_files})\n",
    "json_dataset = Dataset.from_dict({\"filepath\": json_files})\n",
    "\n",
    "print(f\"Processing {len(filtered_screenshot_files)} filtered screenshots...\")\n",
    "screenshot_results = screenshots_dataset.map(\n",
    "    lambda example: process_screenshot(example[\"filepath\"]),\n",
    "    num_proc=num_cores, batched=False, desc=\"Processing screenshots\"\n",
    ")\n",
    "\n",
    "print(f\"Processing {len(json_files)} JSON files...\")\n",
    "json_results = json_dataset.map(\n",
    "    lambda example: process_json_file(example[\"filepath\"]),\n",
    "    num_proc=num_cores, batched=False, desc=\"Processing JSON files\"\n",
    ")\n",
    "\n",
    "# Extract statistics\n",
    "success_screenshots = sum(1 for result in screenshot_results if result[\"status\"] == \"success\")\n",
    "error_screenshots = sum(1 for result in screenshot_results if result[\"status\"] == \"error\")\n",
    "success_json = sum(1 for result in json_results if result[\"status\"] == \"success\")\n",
    "error_json = sum(1 for result in json_results if result[\"status\"] == \"error\")\n",
    "files_with_nonnormalized_coords = sum(1 for result in json_results if result.get(\"normalized\", False))\n",
    "total_events_normalized = sum(result.get(\"events_normalized\", 0) for result in json_results)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== Normalization Summary =====\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Screenshots filtered: {len(filtered_screenshot_files)} out of {len(screenshot_files)} ({filtering_ratio:.2f}%)\")\n",
    "print(f\"Screenshots normalized: {success_screenshots} (Errors: {error_screenshots})\")\n",
    "print(f\"JSON files processed: {success_json} (Errors: {error_json})\")\n",
    "print(f\"Files with coordinates normalized: {files_with_nonnormalized_coords}\")\n",
    "print(f\"Total events normalized: {total_events_normalized}\")\n",
    "\n",
    "# Copy session_prompts.log if it exists\n",
    "session_prompts_path = os.path.join(LOGS_DIR, \"session_prompts.log\")\n",
    "if os.path.exists(session_prompts_path):\n",
    "    shutil.copy2(session_prompts_path, os.path.join(OUTPUT_DIR, \"session_prompts.log\"))\n",
    "    print(\"Copied session_prompts.log to output directory\")\n",
    "\n",
    "print(\"\\nNormalization complete! The normalized data is ready for use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
