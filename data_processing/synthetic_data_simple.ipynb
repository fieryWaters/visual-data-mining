{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6732305-c161-49be-9ce5-d678c24ae26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing directory: synthetic_dataset\n",
      "Processed 4000 samples...\n",
      "Processed 8000 samples...\n",
      "Processed 12000 samples...\n",
      "Processed 5000 samples...\n",
      "Processed 1000 samples...\n",
      "Processed 9000 samples...\n",
      "Processed 13000 samples...\n",
      "Processed 14000 samples...\n",
      "Processed 6000 samples...\n",
      "Processed 10000 samples...\n",
      "Processed 2000 samples...\n",
      "Processed 7000 samples...\n",
      "Processed 11000 samples...\n",
      "Processed 15000 samples...\n",
      "Processed 3000 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a1adcd2d4d4174a90314b0e282ff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/16001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84a758df2704531b9f4a9a704921c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/16002 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b43cfd680e428bbfe7218a4f4f8546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4032e82f314bfc8cfd99437e4d238f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset verification:\n",
      "First example texts type: <class 'list'>\n",
      "First example texts content: [{'assistant': '[349,214]', 'source': 'synthetic_generator', 'user': \"Return the coordinates of the white circle in the image. It's used to locate points of interest and if we click it we can select the circle for further analysis.\"}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2d2f44460043cd94da45ceed140c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e160d37385646eca78fab718b5d60a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a849a4814935437cb1c8be868fe7ffa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed dataset to jwaters8978/synthetic_dataset\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import os, csv, random, json, shutil\n",
    "from PIL import Image, ImageDraw\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "def generate_images(start_idx, end_idx, image_width, image_height, radius, image_dir):\n",
    "    results = []\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Create image\n",
    "        img = Image.new(\"RGB\", (image_width, image_height), \"black\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Generate random coordinates for circle center\n",
    "        x = random.randint(radius, image_width - radius)\n",
    "        y = random.randint(radius, image_height - radius)\n",
    "        \n",
    "        # Draw the circle\n",
    "        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=\"white\")\n",
    "        \n",
    "        # Save image\n",
    "        image_name = f\"image_{i}.png\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img.save(image_path)\n",
    "        \n",
    "        # Create instruction similar to the reference dataset\n",
    "        instruction = f\"Return the coordinates of the white circle in the image. It's used to locate points of interest and if we click it we can select the circle for further analysis.\"\n",
    "        \n",
    "        # Format the data exactly like the reference dataset - as a Python list of dict, not JSON\n",
    "        messages = [\n",
    "            {\n",
    "                \"assistant\": f\"[{x},{y}]\",\n",
    "                \"source\": \"synthetic_generator\",\n",
    "                \"user\": instruction\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Store the messages as a repr string to preserve Python structure\n",
    "        results.append((image_name, repr(messages)))\n",
    "        \n",
    "        # Print progress every 1000 samples\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(f\"Processed {i} samples...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    username = \"jwaters8978\"\n",
    "    repo_name = \"synthetic_dataset\"\n",
    "    num_samples = 16000  \n",
    "    image_width, image_height = 1280, 720\n",
    "    radius = 4\n",
    "    \n",
    "    # Setup directories\n",
    "    dataset_dir = \"synthetic_dataset\"\n",
    "    \n",
    "    # Clean up existing directory if it exists\n",
    "    if os.path.exists(dataset_dir):\n",
    "        print(f\"Removing existing directory: {dataset_dir}\")\n",
    "        shutil.rmtree(dataset_dir)\n",
    "    \n",
    "    # Create fresh directories\n",
    "    image_dir = os.path.join(dataset_dir, \"train\", \"class0\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    \n",
    "    # Determine how many processes and split work\n",
    "    num_processes = 100\n",
    "    chunk_size = num_samples // num_processes\n",
    "    ranges = [(i*chunk_size, (i+1)*chunk_size if i < num_processes-1 else num_samples)\n",
    "              for i in range(num_processes)]\n",
    "    \n",
    "    # Prepare arguments for multiprocessing\n",
    "    args = [(start, end, image_width, image_height, radius, image_dir) for start, end in ranges]\n",
    "    \n",
    "    # Generate images and metadata using multiple processes\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results_list = pool.starmap(generate_images, args)\n",
    "    \n",
    "    # Flatten results\n",
    "    all_results = [item for sublist in results_list for item in sublist]\n",
    "    \n",
    "    # Write metadata to the same directory as the images\n",
    "    metadata_path = os.path.join(dataset_dir, \"train\", \"class0\", \"metadata.csv\")\n",
    "    with open(metadata_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"file_name\", \"texts\"])\n",
    "        writer.writerows(all_results)\n",
    "    \n",
    "    # Load and push the dataset\n",
    "    dataset = load_dataset(\"imagefolder\", data_dir=dataset_dir)\n",
    "    dataset = dataset.rename_column('image', 'images')\n",
    "    \n",
    "    # Add function to convert string representation back to Python object\n",
    "    def convert_texts(example):\n",
    "        example['texts'] = eval(example['texts'])\n",
    "        return example\n",
    "    \n",
    "    # Apply the conversion\n",
    "    dataset = dataset.map(convert_texts)\n",
    "    \n",
    "    print(\"\\nDataset verification:\")\n",
    "    print(\"First example texts type:\", type(dataset['train'][0]['texts']))\n",
    "    print(\"First example texts content:\", dataset['train'][0]['texts'])\n",
    "    \n",
    "    # Push to HuggingFace\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id=f\"{username}/{repo_name}\", repo_type=\"dataset\", exist_ok=True)\n",
    "    dataset.push_to_hub(f\"{username}/{repo_name}\")\n",
    "    print(f\"Pushed dataset to {username}/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c76ddb5-b264-4b54-82df-184a3f1466e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Dataset Analysis ===\n",
      "\n",
      "Original dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['images', 'texts'],\n",
      "        num_rows: 15937\n",
      "    })\n",
      "})\n",
      "\n",
      "Column names: ['images', 'texts']\n",
      "\n",
      "First example:\n",
      "Text type: <class 'list'>\n",
      "Text content: [{'assistant': '[79.92, 7.64, 85.31, 10.42]', 'source': 'web_scraper', 'user': \"Return the bounding box of the Text link with the words 'About Us'. It's used to navigate to the About Us section of the website and if we click it the About Us section will load.\"}]\n",
      "\n",
      "=== Synthetic Dataset Analysis ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f34feebba3c4db58732fc69f773ce4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/312 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f4850f04134c52a431bdfb66be8009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/831k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27e98140c8f4e3ba7bd1d6296b03a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['images', 'texts'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "\n",
      "Column names: ['images', 'texts']\n",
      "\n",
      "First example:\n",
      "Text type: <class 'str'>\n",
      "Text content: [{\"assistant\": \"[1116,349]\", \"source\": \"synthetic_generator\", \"user\": \"Return the coordinates of the white circle in the image. It's used to locate points of interest and if we click it we can select the circle for further analysis.\"}]\n"
     ]
    }
   ],
   "source": [
    "# Analysis script for comparing datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the original dataset\n",
    "print(\"=== Original Dataset Analysis ===\")\n",
    "original_dataset = load_dataset(\"jwaters8978/web_scraper_dataset_2\")\n",
    "print(\"\\nOriginal dataset structure:\")\n",
    "print(original_dataset)\n",
    "\n",
    "print(\"\\nColumn names:\", original_dataset['train'].column_names)\n",
    "print(\"\\nFirst example:\")\n",
    "example = original_dataset['train'][0]\n",
    "print(\"Text type:\", type(example['texts']))\n",
    "print(\"Text content:\", example['texts'])\n",
    "\n",
    "# Load our synthetic dataset\n",
    "print(\"\\n=== Synthetic Dataset Analysis ===\")\n",
    "synthetic_dataset = load_dataset(\"jwaters8978/synthetic_dataset\")\n",
    "print(\"\\nSynthetic dataset structure:\")\n",
    "print(synthetic_dataset)\n",
    "\n",
    "print(\"\\nColumn names:\", synthetic_dataset['train'].column_names)\n",
    "print(\"\\nFirst example:\")\n",
    "example = synthetic_dataset['train'][0]\n",
    "if 'texts' in example:\n",
    "    print(\"Text type:\", type(example['texts']))\n",
    "    print(\"Text content:\", example['texts'])\n",
    "else:\n",
    "    print(\"No 'texts' column found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
