{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cde32e-7886-474f-b60e-a50ed0578298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a7d83e-315c-413e-93e7-be28aa8cc732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM, \n",
    "    CLIPProcessor, \n",
    "    CLIPModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face authentication\n",
    "def setup_environment(hf_token):\n",
    "    \"\"\"Initialize Hugging Face authentication and confirm GPU availability\"\"\"\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged into Hugging Face!\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Load required models\n",
    "def load_models(device):\n",
    "    \"\"\"Load all required models\"\"\"\n",
    "    # Load Llama model and processor\n",
    "    model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load CLIP for additional image processing if needed\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    return processor, model, clip_model, clip_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90add57e-2cff-48b8-bd75-538ac7141df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80ee12-f632-443c-8190-d924ced58ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButtonDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def format_button_info(self, item):\n",
    "        \"\"\"Format button information into a structured description\"\"\"\n",
    "        return (\n",
    "            f\"Button Name: {item['name']}\\n\"\n",
    "            f\"Type: {item['type']}\\n\"\n",
    "            f\"Location: {item['bbox']}\\n\"\n",
    "            f\"Purpose: {item['purpose']}\\n\"\n",
    "            f\"Description: {item['description']}\\n\"\n",
    "            f\"Expected Behavior: {item['expectation']}\\n\"\n",
    "            f\"Resolution: {item['resolution']}\"\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(requests.get(item['image'], stream=True).raw).convert('RGB')\n",
    "        \n",
    "        # Create instruction-based prompt\n",
    "        instruction = (\n",
    "            f\"Analyze this UI element with the following instruction: {item['instruction']}\\n\"\n",
    "            f\"Identify and describe the button's properties including its location, \"\n",
    "            f\"purpose, behavior, and visual characteristics.\"\n",
    "        )\n",
    "        \n",
    "        # Format target text with all button details\n",
    "        target_text = self.format_button_info(item)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=instruction,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Process target\n",
    "        target_inputs = self.processor(\n",
    "            text=target_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.squeeze(0)\n",
    "        \n",
    "        inputs[\"labels\"] = target_inputs.input_ids.squeeze(0)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "def prepare_datasets(processor):\n",
    "    \"\"\"Load and prepare the Wave UI dataset\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"miketes/Web-filtered-english-wave-ui-25k\")\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset = ButtonDetectionDataset(dataset['train'], processor)\n",
    "    val_dataset = ButtonDetectionDataset(dataset['validation'], processor)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33d9cf-5b3f-4ce3-86db-d733333dbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e865c4-f137-4328-ab94-5477a15a23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training_args(output_dir):\n",
    "    \"\"\"Configure training arguments\"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        warmup_steps=500,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=\"meta-llama/Llama-3.2-11B-Vision-Instruct\"  # Replace with your desired model name\n",
    "    )\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, training_args):\n",
    "    \"\"\"Initialize trainer and start training\"\"\"\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(\"./button-detection-model-final\")\n",
    "    \n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub()\n",
    "\n",
    "def main(hf_token):\n",
    "    # Setup environment\n",
    "    device = setup_environment(hf_token)\n",
    "    \n",
    "    # Load models\n",
    "    processor, model, clip_model, clip_processor = load_models(device)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset = prepare_datasets(processor)\n",
    "    \n",
    "    # Setup training arguments\n",
    "    training_args = setup_training_args(\"./button-detection-model\")\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model, train_dataset, val_dataset, training_args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    HF_TOKEN = \"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\"  # Replace with your token\n",
    "    main(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a493a9f-ff5b-4131-b050-1867cd950fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inference and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e98a1-6f7f-4628-a5a2-8b7ef2f94093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fine_tuned_model(model_path, device):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    return processor, model\n",
    "\n",
    "def predict_button_details(processor, model, image_path, instruction=None):\n",
    "    \"\"\"Generate predictions for a given image\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    if instruction is None:\n",
    "        instruction = \"Describe the button in this image, including its location, purpose, and expected behavior.\"\n",
    "    \n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=instruction,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(processor, model, test_dataset, num_samples=10):\n",
    "    \"\"\"Evaluate model performance on test samples\"\"\"\n",
    "    results = []\n",
    "    for i in range(num_samples):\n",
    "        sample = test_dataset[i]\n",
    "        \n",
    "        prediction = predict_button_details(\n",
    "            processor,\n",
    "            model,\n",
    "            sample['image'],\n",
    "            sample['instruction']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'instruction': sample['instruction'],\n",
    "            'ground_truth': {\n",
    "                'name': sample['name'],\n",
    "                'type': sample['type'],\n",
    "                'bbox': sample['bbox'],\n",
    "                'purpose': sample['purpose'],\n",
    "                'description': sample['description'],\n",
    "                'expectation': sample['expectation'],\n",
    "                'resolution': sample['resolution']\n",
    "            },\n",
    "            'prediction': prediction\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "def run_inference(model_path, image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    processor, model = load_fine_tuned_model(model_path, device)\n",
    "    \n",
    "    result = predict_button_details(\n",
    "        processor,\n",
    "        model,\n",
    "        image_path,\n",
    "        \"Analyze this UI element and describe its properties.\"\n",
    "    )\n",
    "    \n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
